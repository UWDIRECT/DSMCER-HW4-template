{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSMCER HW 4\n",
    "\n",
    "###  Part 1 Decision tree regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the HCEPDB file you have downloaded with your `setup.sh` script from SEDS-HW1.\n",
    "\n",
    "* 1.1 Randomly extract 2% of the points from the HCEPDB file. This subset will be used as your input data. \n",
    "* 1.2 Define a training and testing set from the subset you just extracted. When defining X, use the ['mass', 'e_homo_alpha', 'voc', 'pce', 'jsc'] columns only. Your target output, Y, should be the 'e_gap_alpha' column.\n",
    "* 1.3 Train a Decision tree using the top down greedy approach on your training set. Predict the test set and compute the MSE on your test prediction. Print the MSE.\n",
    "* 1.4 Repeat the above steps (1.2 and 1.3) for each of max_depth = [2, 5, 10, 50, 100] and store the MSE in a list. \n",
    "* 1.5 Plot the natural logarithm of the MSE list as a function of max depth. Clearly label your lines and axes.\n",
    "* 1.6 In a markdown cell, explain the trend of the MSE in your plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part 2 Bagging, random forest and gradient boosting decision tree regression\n",
    "\n",
    "Consider this list of numbers of trees [5, 10, 20, 50, 100, 300, 1000]. For each number of trees in the list:\n",
    "   * 2.1 Define a Bagging regressor, train it using the train set from part 1 and predict using the test set from part 1. Save the MSE at each iteration of numbers of trees in a list. Use a maximum depth of 4. \n",
    "   * 2.2 Define a Random Forest regressor, train it using the train set from part 1 and predict using the test set from part 1. Save the MSE at each iteration of numbers of trees in a list. Use a maximum depth of 4. Here set the maximum features using the next closest integer to the square root of the number of parameters.\n",
    "   * 2.3 Define a Gradient Boosting regressor, train it using the train set from part 1 and predict using the test set from part 1. Save the MSE at each iteration of numbers of trees in a list. Use a maximum depth of 1.\n",
    "   * 2.4 Plot the natural logarithm of the MSE lists obtained from each regressor as a function of numbers of trees. Clearly label your lines and axes.\n",
    "   * 2.5 In a markdown cell discuss the trends that you observe in your plot. How would these change if you changed the maximum number of features in your random forest algorithm?\n",
    "\n",
    "You can find information on the functions your will need [here](https://scikit-learn.org/stable/modules/ensemble.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (tunnel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
